{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrasanthBalaraman/MachineLearning/blob/master/SmartSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jRwKEKGVNj2H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "fc1d71d5-5826-4228-9614-84493ac6aa25"
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/a4/d10c0acc8528d838cda5eede0ee9c784caa598dbf40bd0911ff8d067a7eb/gensim-3.6.0-cp36-cp36m-manylinux1_x86_64.whl (23.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 23.6MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Collecting smart-open>=1.2.1 (from gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/1f/6f27e3682124de63ac97a0a5876da6186de6c19410feab66c1543afab055/smart_open-1.7.1.tar.gz\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 14.1MB/s \n",
            "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/5e/d39501836d6b3a198fc8ca34ca058f82f555c0e48b7a929f972cfc066e99/boto3-1.9.53-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 29.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.10.15)\n",
            "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 23.1MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
            "Collecting botocore<1.13.0,>=1.12.53 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/f6/1b481cef9ad9978b8c8e57713416af5b041d7742912087188fc46a638480/botocore-1.12.53-py2.py3-none-any.whl (5.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.0MB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.53->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Collecting docutils>=0.10 (from botocore<1.13.0,>=1.12.53->boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 21.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: smart-open, bz2file\n",
            "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/23/00/44/e5b939f7a80c04e32297dbd6d96fa3065af89ecf57e2b5f89f\n",
            "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "Successfully built smart-open bz2file\n",
            "Installing collected packages: boto, bz2file, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
            "Successfully installed boto-2.49.0 boto3-1.9.53 botocore-1.12.53 bz2file-0.98 docutils-0.14 gensim-3.6.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l88tbSWUNxWX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7153144f-c86d-484e-b01e-6fb94ded4a1c"
      },
      "cell_type": "code",
      "source": [
        "import nltk \n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "_uHTKyTZJBXo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import collections\n",
        "import pickle\n",
        "import numpy as np\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "train_article_path =\"C:\\\\Users\\\\muvaiz\\\\Documents\\\\Genesys_Demojam\\\\Code\\\\sample_data\\\\sumdata\\\\train\\\\train.article.txt\"\n",
        "train_title_path =\"C:\\\\Users\\\\muvaiz\\\\Documents\\\\Genesys_Demojam\\\\Code\\\\sample_data\\\\sumdata\\\\train\\\\train.title.txt\"\n",
        "#valid_article_path =\"C:\\\\Users\\\\muvaiz\\\\Documents\\\\Genesys_Demojam\\\\Code\\\\sample_data\\\\sumdata\\\\train\\\\valid.article.filter.txt\"\n",
        "#valid_title_path =\"C:\\\\Users\\\\muvaiz\\\\Documents\\\\Genesys_Demojam\\\\Code\\\\sample_data\\\\sumdata\\\\train\\\\valid.title.filter.txt\"\n",
        "valid_article_path =\"C:\\\\Users\\\\muvaiz\\\\Documents\\\\Genesys_Demojam\\\\Code\\\\sample_data\\\\sumdata\\\\train\\\\test.article.txt\"\n",
        "\n",
        "#to clean sentence\n",
        "def clean_str(sentence):\n",
        "    sentence=re.sub(\"[#.]+\",\"#\",sentence)\n",
        "    return sentence\n",
        "\n",
        "#read each line from given file and will pass it to clean_str function for cleaning sentence. \"toy\" flag will be used for optimization\n",
        "def get_text_list(data_path,toy):\n",
        "    with open(data_path,'r',encoding=\"utf-8\") as f:\n",
        "        if not toy:\n",
        "            return([clean_str(x.strip()) for x in f.readlines()])\n",
        "        else:\n",
        "            return([clean_str(x.strip()) for x in f.readlines()][:50000])\n",
        "      \n",
        "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
        "    inputs = np.array(inputs)\n",
        "    outputs = np.array(outputs)\n",
        "\n",
        "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
        "            yield inputs[start_index:end_index], outputs[start_index:end_index]\n",
        "            \n",
        "def get_init_embedding(reversed_dict,embedding_size):\n",
        "    print(\"Using GloVe\")\n",
        "    glove_file= 'glove.6B.200d.txt'  #\"C:\\\\Users\\\\muvaiz\\\\Documents\\\\Python_Scripts\\\\LSTM\\\\glove.6B\\\\glove.6B.50d.txt\"\n",
        "    word2vec_file=get_tmpfile(\"word2vec_format.vec\")\n",
        "    glove2word2vec(glove_file,word2vec_file)\n",
        "    print(\"Loading Glove vectors...\")\n",
        "    word_vectors = KeyedVectors.load_word2vec_format(word2vec_file)\n",
        "    \n",
        "    word_vec_list = list()\n",
        "    \n",
        "    for _,word in sorted(reversed_dict.items()):\n",
        "        try:\n",
        "            word_vec = word_vectors.word_vec(word)\n",
        "        except KeyError:\n",
        "            \n",
        "            word_vec=np.zeros([embedding_size],dtype=np.float32)\n",
        "            \n",
        "        word_vec_list.append(word_vec)\n",
        "        \n",
        "    word_vec_list[2] = np.random.normal(0, 1, embedding_size)\n",
        "    word_vec_list[3] = np.random.normal(0, 1, embedding_size)\n",
        "    \n",
        "    return(np.array(word_vec_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EwAVUQxkNbCT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "5721fe03-42b7-4411-cdcf-d2945ab9efbb"
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import collections\n",
        "import pickle\n",
        "import numpy as np\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "# load from file\n",
        "stories = pickle.load(open('cnn_dataset.pkl', 'rb'))\n",
        "print('Loaded Stories %d' % len(stories))\n",
        "\n",
        "def print_story(stories):\n",
        "  index = np.random.randint(90000)\n",
        "  story, highlights = stories[index]['story'], stories[index]['highlights']\n",
        "  print(\"\\nSample Story\\n\")\n",
        "  print(\"Story: \\n\")\n",
        "  print(story)\n",
        "  print(\"Highlights: \\n\")\n",
        "  print(highlights)\n",
        "  print(\"\\n\")\n",
        "print_story(stories)\n",
        "\n",
        "def build_dict(step,stories,toy=False):\n",
        "    if step==\"train\":\n",
        "        #train_article_list=get_text_list(train_article_path,toy)\n",
        "        #train_title_list=get_text_list(train_title_path,toy)\n",
        "        train_article_list=list()\n",
        "        train_title_list=list()\n",
        "        for each_story_highlight in stories:\n",
        "            train_article_list.append(each_story_highlight['story'])\n",
        "            train_title_list.append(each_story_highlight['highlights'])\n",
        "        \n",
        "        words=list()\n",
        "        \n",
        "        for paragraph in train_article_list:\n",
        "            for sentence in paragraph:\n",
        "                for word in word_tokenize(sentence):\n",
        "                    words.append(word)\n",
        "                \n",
        "        word_counter=collections.Counter(words).most_common()\n",
        "        word_dict=dict()\n",
        "        word_dict[\"<padding>\"]=0\n",
        "        word_dict[\"<unk>\"]=1\n",
        "        word_dict[\"<s>\"]=2\n",
        "        word_dict[\"</s>\"]=3\n",
        "        \n",
        "        for word,_ in word_counter:\n",
        "            word_dict[word]=len(word_dict)\n",
        "            \n",
        "        with open(\"word_dict.pickle\",\"wb\") as f:\n",
        "            pickle.dump(word_dict,f)\n",
        "            \n",
        "    elif step==\"valid\":\n",
        "        with open(\"word_dict.pickle\",\"rb\") as f:\n",
        "            word_dict=pickle.load(f)\n",
        "            \n",
        "    reversed_dict = dict(zip(word_dict.values(),word_dict.keys()))\n",
        "    \n",
        "    article_max_len=50\n",
        "    summary_max_len=15\n",
        "    \n",
        "    return word_dict,reversed_dict,article_max_len,summary_max_len\n",
        "  \n",
        "\n",
        "def build_dataset(step,stories,word_dict,article_max_len,summary_max_len,toy=False):\n",
        "    if step==\"train\":\n",
        "        article_list=list()\n",
        "        title_list=list()\n",
        "        for each_story_highlight in stories:\n",
        "            article_list.append(each_story_highlight['story'])\n",
        "            train_title_list.append(each_story_highlight['highlights'])\n",
        "        article_list=get_text_list(train_article_path,toy)\n",
        "        title_list=get_text_list(train_title_path,toy)\n",
        "    elif step==\"valid\":\n",
        "        article_list=get_text_list(valid_article_path,toy)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    #sample output of sentence_with_word_lists_per_sent : [[\"I\",\"am\",\"a\",\"boy\"],[\"I\",\"like\",\"rain\"]]\n",
        "    sentences_with_word_lists_per_sent=[list(map(word_tokenize, paragraph)) for paragraph in article_list]\n",
        "    \n",
        "    #sample output of sentence_with_word_lists_per_sent : [[\"101\",\"21\",\"343\",\"711\"],[\"100\",\"234\",\"1526\"]]\n",
        "    sentences_with_word_vector_lists_per_sent=[[word_dict.get(word,word_dict[\"<unk>\"]) for word in sentence] for sentence in sentences_with_word_lists_per_sent]\n",
        "                                                              \n",
        "    truncated_word_vector_lists_per_sent=[sentence[:article_max_len] for sentence in sentences_with_word_vector_lists_per_sent]\n",
        "    output_word_vector_lists=[sentence + (article_max_len - len(sentence)) * [word_dict[\"<padding>\"]] for sentence in truncated_word_vector_lists_per_sent]\n",
        "                                                              \n",
        "    if step==\"valid\":\n",
        "        return output_word_vector_lists\n",
        "    else:\n",
        "        title_with_word_lists_per_sent = [word_tokenize(sentence) for sentence in title_list]\n",
        "        title_with_word_vector_lists_per_sent=[[word_dict.get(word,word_dict[\"<unk>\"]) for word in sentence] for sentence in title_with_word_lists_per_sent]\n",
        "        output_title_vector_lists=[sentence[:(summary_max_len-1)]for sentence in title_with_word_vector_lists_per_sent]\n",
        "        return(output_word_vector_lists,output_title_vector_lists)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Stories 92579\n",
            "\n",
            "Sample Story\n",
            "\n",
            "Story: \n",
            "\n",
            "['hes fluent in both english and arabic and if his college degree in computers is any indication he has a way with technology then theres his interest in radical islam', 'put it all together and authorities say former boston resident and us citizen ahmad abousamra could be a good fit inside the isis social media machine thats become renowned in recent weeks for spewing brutal propaganda across social media messages meant both to terrify and recruit westerners', 'theres no specific evidence pointing to abousamras involvement in the terror groups social media campaign but federal investigators think he may have joined the group and is helping with online efforts a law enforcement official told cnn', 'abousamra who holds dual us and syrian citizenship is wanted by the fbi on terrorism charges issued in they include providing material support to terrorists theres a reward for his capture', 'according to the fbi abousamra repeatedly traveled to pakistan and yemen seeking military training for the purpose of killing american soldiers overseas', 'he returned to the united states but left after fbi terrorism investigators questioned him', 'he was last believed to be living in syria according to the fbi', 'according to abc abousamra grew up in boston his father was a doctor and he went to a private catholic high school and a public school before attending northeastern university where he made the deans list according to abc which cited school officials', 'showing off its crimes how isis flaunts its brutality as propaganda', 'isis appears to have a wellfunded wellorganized social media and video production effort its videos are slickly produced with high production values experts say and isis imagery has proven tough to stamp out', 'twitter lagged behind isis on efforts to remove images of the decapitation of us journalists cnn national security analyst fran townsend noted', 'weve seen propaganda campaigns from al qaeda before but nothing that acts that quickly in realtime events thats able to target tactically and thats a real concern to american officials she said', 'juliette kayyem also a cnn national security analyst said social media is helping isis which also calls itself the islamic state amplify its message', 'they are a serious threat and they are using social media to make themselves even bigger in that regard she said', 'social media battle augments iraq bloodshed', 'cnn exclusive from glasgow girl to bedroom radical and isis bride']\n",
            "Highlights: \n",
            "\n",
            "['us investigators looking to see if ahmad abousamra is involved a law enforcement official says', 'abousamra is wanted on federal terrorism charges issued in', 'he traveled to pakistan and yemen seeking terror training fbi says', 'he also has a computer degree and is fluent in english and arabic fbi says']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RqtsbzQlpZDC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2bd3c0be-9dea-45dc-a8bc-dc67c74c944e"
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Nov 26 23:47:16 2018\n",
        "\n",
        "@author: muvaiz\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "start = time.perf_counter()\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import pickle\n",
        "import os\n",
        "#from model import Model\n",
        "#from utils import build_dict, build_dataset, batch_iter\n",
        "\n",
        "class Simulate_Arguments():\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.num_hidden=150\n",
        "    self.num_layers=2\n",
        "    self.beam_width=10\n",
        "    self.glove=True\n",
        "    self.embedding_size=200\n",
        "    \n",
        "    self.learning_rate=1e-3\n",
        "    self.batch_size=64\n",
        "    self.num_epochs=10\n",
        "    self.keep_prob=0.8\n",
        "    \n",
        "    self.toy=True\n",
        "    self.with_model=False\n",
        "\n",
        "# Uncomment next 2 lines to suppress error and Tensorflow info verbosity. Or change logging levels\n",
        "# tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# def add_arguments(parser):\n",
        "#     parser.add_argument(\"--num_hidden\", type=int, default=150, help=\"Network size.\")\n",
        "#     parser.add_argument(\"--num_layers\", type=int, default=2, help=\"Network depth.\")\n",
        "#     parser.add_argument(\"--beam_width\", type=int, default=10, help=\"Beam width for beam search decoder.\")\n",
        "#     parser.add_argument(\"--glove\", action=\"store_true\", help=\"Use glove as initial word embedding.\")\n",
        "#     parser.add_argument(\"--embedding_size\", type=int, default=300, help=\"Word embedding size.\")\n",
        "\n",
        "#     parser.add_argument(\"--learning_rate\", type=float, default=1e-3, help=\"Learning rate.\")\n",
        "#     parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size.\")\n",
        "#     parser.add_argument(\"--num_epochs\", type=int, default=1, help=\"Number of epochs.\")\n",
        "#     parser.add_argument(\"--keep_prob\", type=float, default=0.8, help=\"Dropout keep prob.\")\n",
        "\n",
        "#     parser.add_argument(\"--toy\", action=\"store_true\", help=\"Use only 50K samples of data\")\n",
        "\n",
        "#     parser.add_argument(\"--with_model\", action=\"store_true\", help=\"Continue from previously saved model\")\n",
        "\n",
        "\n",
        "\n",
        "#parser = argparse.ArgumentParser()\n",
        "#add_arguments(parser)\n",
        "args = Simulate_Arguments()\n",
        "with open(\"args.pickle\", \"wb\") as f:\n",
        "    pickle.dump(args, f)\n",
        "\n",
        "if not os.path.exists(\"saved_model\"):\n",
        "    os.mkdir(\"saved_model\")\n",
        "else:\n",
        "    if args.with_model:\n",
        "        old_model_checkpoint_path = open('saved_model/checkpoint', 'r')\n",
        "        old_model_checkpoint_path = \"\".join([\"saved_model/\",old_model_checkpoint_path.read().splitlines()[0].split('\"')[1] ])\n",
        "\n",
        "# load from file\n",
        "stories = pickle.load(open('cnn_dataset.pkl', 'rb'))\n",
        "print('Loaded Stories %d' % len(stories))\n",
        "        \n",
        "print(\"Building dictionary...\")\n",
        "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\",stories, args.toy)\n",
        "print(\"Loading training dataset...\")\n",
        "train_x, train_y = build_dataset(\"train\", stories, word_dict, article_max_len, summary_max_len, args.toy)\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    model = Model(reversed_dict, article_max_len, summary_max_len, args)\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver(tf.global_variables())\n",
        "    if 'old_model_checkpoint_path' in globals():\n",
        "        print(\"Continuing from previous trained model:\" , old_model_checkpoint_path , \"...\")\n",
        "        saver.restore(sess, old_model_checkpoint_path )\n",
        "\n",
        "    batches = batch_iter(train_x, train_y, args.batch_size, args.num_epochs)\n",
        "    num_batches_per_epoch = (len(train_x) - 1) // args.batch_size + 1\n",
        "\n",
        "    print(\"\\nIteration starts.\")\n",
        "    print(\"Number of batches per epoch :\", num_batches_per_epoch)\n",
        "    for batch_x, batch_y in batches:\n",
        "        batch_x_len = list(map(lambda x: len([y for y in x if y != 0]), batch_x))\n",
        "        batch_decoder_input = list(map(lambda x: [word_dict[\"<s>\"]] + list(x), batch_y))\n",
        "        batch_decoder_len = list(map(lambda x: len([y for y in x if y != 0]), batch_decoder_input))\n",
        "        batch_decoder_output = list(map(lambda x: list(x) + [word_dict[\"</s>\"]], batch_y))\n",
        "\n",
        "        batch_decoder_input = list(\n",
        "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_input))\n",
        "        batch_decoder_output = list(\n",
        "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_output))\n",
        "\n",
        "        train_feed_dict = {\n",
        "            model.batch_size: len(batch_x),\n",
        "            model.X: batch_x,\n",
        "            model.X_len: batch_x_len,\n",
        "            model.decoder_input: batch_decoder_input,\n",
        "            model.decoder_len: batch_decoder_len,\n",
        "            model.decoder_target: batch_decoder_output\n",
        "        }\n",
        "\n",
        "        _, step, loss = sess.run([model.update, model.global_step, model.loss], feed_dict=train_feed_dict)\n",
        "\n",
        "        if step % 1000 == 0:\n",
        "            print(\"step {0}: loss = {1}\".format(step, loss))\n",
        "\n",
        "        if step % num_batches_per_epoch == 0:\n",
        "            hours, rem = divmod(time.perf_counter() - start, 3600)\n",
        "            minutes, seconds = divmod(rem, 60)\n",
        "            saver.save(sess, \"./saved_model/model.ckpt\", global_step=step)\n",
        "            print(\" Epoch {0}: Model is saved.\".format(step // num_batches_per_epoch),\n",
        "            \"Elapsed: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds) , \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Stories 92579\n",
            "Building dictionary...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}